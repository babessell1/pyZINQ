{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bbessell\\Projects\\GitHub\\pyZINQ\n",
      "['.git', '.gitignore', 'License.md', 'py', 'README.md', 'refs', 'tests']\n"
     ]
    }
   ],
   "source": [
    "# set working directory\n",
    "import os\n",
    "os.chdir('..')\n",
    "from py.firth import firth_logistic_regression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(os.getcwd())\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   covariate      beta   std_err       fitll      wald     pvals\n",
      "0  intercept  0.120255  0.485542 -138.455068  0.247672  0.804388\n",
      "1        age -1.105989  0.423662 -138.455068  2.610544  0.009040\n",
      "2         oc -0.068815  0.443794 -138.455068  0.155062  0.876773\n",
      "3        vic  2.268876  0.548417 -138.455068  4.137137  0.000035\n",
      "4       vicl -2.111412  0.543084 -138.455068  3.887820  0.000101\n",
      "5        vis -0.788319  0.417368 -138.455068  1.888785  0.058921\n",
      "6        dia  3.096556  1.675363 -138.455068  1.848290  0.064560\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"refs/uti_data.csv\", delimiter=\";\").drop(columns=[\"Unnamed: 0\"])\n",
    "data.head()\n",
    "\n",
    "covariates = [\"age\", \"oc\", \"vic\", \"vicl\", \"vis\", \"dia\"]\n",
    "X_ = data[covariates].to_numpy().astype(np.float64)\n",
    "y_ = data[\"case\"].to_numpy().astype(np.float64)\n",
    "\n",
    "beta, std_err, fitll, wald, pvals = firth_logistic_regression(y_, X_, tol=1e-8, test='wald')\n",
    "summary = pd.DataFrame({\n",
    "    \"covariate\": [\"intercept\"] + covariates,\n",
    "    \"beta\": beta,\n",
    "    \"std_err\": std_err,\n",
    "    \"fitll\": fitll,\n",
    "    \"wald\": wald,\n",
    "    \"pvals\": pvals\n",
    "})\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output of R's logistf package:\n",
    "'''\n",
    "logistf(formula = case ~ age + oc + vic + vicl + vis + dia, data = sex2)\n",
    "\n",
    "Model fitted by Penalized ML\n",
    "method\n",
    "                   coef  se(coef) lower 0.95  upper 0.95       Chisq            p\n",
    "(Intercept)  0.12025405 0.4763429 -0.8133609  1.05386904  0.06373235 8.006912e-01      1\n",
    "age         -1.10598131 0.4149021 -1.9191744 -0.29278818  7.10565893 7.684097e-03      1\n",
    "oc          -0.06881673 0.4344026 -0.9202301  0.78259664  0.02509593 8.741283e-01      1\n",
    "vic          2.26887464 0.5384872  1.2134590  3.32429026 17.75293469 2.515292e-05      1\n",
    "vicl        -2.11140817 0.5320395 -3.1541864 -1.06862995 15.74913385 7.232104e-05      1\n",
    "vis         -0.78831694 0.4089620 -1.5898677  0.01323384  3.71565877 5.390435e-02      1\n",
    "dia          3.09601166 1.5052197  0.1458353  6.04618801  4.23063350 3.970062e-02\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff betas:  [ 1.07547291e-06 -7.31802781e-06  1.28657029e-06  1.49313811e-06\n",
      " -3.60615652e-06 -1.72431510e-06  5.44770210e-04]\n",
      "diff pvals:  [3.69698066e-03 1.35573955e-03 2.64434182e-03 1.00137227e-05\n",
      " 2.88275230e-05 5.01624583e-03 2.48597815e-02]\n",
      "pass\n"
     ]
    }
   ],
   "source": [
    "# unit test\n",
    "correct_betas = [0.12025405, -1.10598131, -0.06881673, 2.26887464, -2.11140817, -0.78831694, 3.09601166]\n",
    "correct_pvals = [8.006912e-01, 7.684097e-03, 8.741283e-01, 2.515292e-05, 7.232104e-05, 5.390435e-02, 3.970062e-02]\n",
    "\n",
    "print(\"diff betas: \", beta - correct_betas)\n",
    "print(\"diff pvals: \", pvals - correct_pvals)\n",
    "\n",
    "assert np.allclose(beta, correct_betas, atol=1e-3)\n",
    "assert np.allclose(pvals, correct_pvals, atol=5e-2)\n",
    "print(\"pass\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.722841800001333\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "print(timeit.timeit(lambda: firth_logistic_regression(y_, X_, test='wald'), number=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9583333333333334"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(23/100)*15000/60/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP WIP WIP WIP\n",
    "\n",
    "import numpy as np\n",
    "from numba import njit\n",
    "from scipy import special, linalg\n",
    "import warnings\n",
    "\n",
    "@njit\n",
    "def logit(p):\n",
    "    return np.log(p / (1 - p))\n",
    "\n",
    "@njit\n",
    "def logistic(x, eps=1e-5):\n",
    "    \"\"\" Compute logistic function, clipping the output to be within (eps, 1-eps). \"\"\"\n",
    "    # Add clipping to avoid values exactly at 0 or 1\n",
    "    return np.clip(1 / (1 + np.exp(-x)), eps, 1 - eps)\n",
    "\n",
    "@njit\n",
    "def compute_log_likelihood(beta, X, y):\n",
    "    logits = X @ beta\n",
    "    log_likelihood = np.sum(y * logits - np.log(1 + np.exp(logits)))\n",
    "    return log_likelihood\n",
    "\n",
    "@njit\n",
    "def firth_penalty(log_likelihood, H):\n",
    "    sign, logdet = np.linalg.slogdet(-H)\n",
    "    return -(log_likelihood + 0.5 * logdet)\n",
    "\n",
    "@njit\n",
    "def compute_fisher_information(X, pi, eps=np.finfo(float).eps):\n",
    "    \"\"\" Compute Fisher information matrix, ensuring numerical stability. \"\"\"\n",
    "    # Use an epsilon value to ensure pi * (1 - pi) does not result in zero rows\n",
    "    W = np.diag(np.clip(pi * (1 - pi), eps, None))  # Add a small epsilon\n",
    "    H = X.T @ (W @ X)\n",
    "    return np.linalg.pinv(H)\n",
    "\n",
    "@njit\n",
    "def compute_score(X, y, pi, H):\n",
    "    root_diag_h = np.sqrt(np.diag(X @ (np.linalg.pinv(-H) @ X.T)))\n",
    "    delta_pi = root_diag_h * (0.5 - pi)\n",
    "    U = X.T @ (y - pi + delta_pi)\n",
    "    return U\n",
    "\n",
    "@njit\n",
    "def compute_standard_errors(inverse_fisher_info):\n",
    "    return np.sqrt(np.diag(inverse_fisher_info))\n",
    "\n",
    "@njit\n",
    "def compute_wald_test(beta, standard_errors):\n",
    "    wald_stats = np.square(beta / standard_errors)\n",
    "    p_values = 2 * (1 - norm.cdf(np.abs(beta / standard_errors)))  # two-tailed\n",
    "\n",
    "    return wald_stats, p_values\n",
    "\n",
    "@njit\n",
    "def compute_likelihood_ratio_test(full_ll, reduced_ll):\n",
    "    lr_stat = -2 * (reduced_ll - full_ll)\n",
    "    p_value = chi2.sf(lr_stat, df=1)  # df=1 for a single parameter test\n",
    "    return lr_stat, p_value\n",
    "\n",
    "\n",
    "def firth_logistic_regression(y, X, max_iter=1000, tol=1e-6, test='wald'):\n",
    "    N, p = X.shape\n",
    "    beta = np.zeros(p+1)\n",
    "    X = np.hstack((np.ones((N, 1)), X))  # Adding a column for the intercept\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        pi = logistic(X @ beta)\n",
    "        H = compute_fisher_information(X, pi)\n",
    "        ll = compute_log_likelihood(beta, X, y)\n",
    "\n",
    "        if firth_penalty(ll, H) < tol:\n",
    "            break\n",
    "\n",
    "        U = compute_score(X, y, pi, H)\n",
    "\n",
    "        inverse_H = linalg.pinv(-H)  # Inverting the Hessian matrix\n",
    "        print(\"Shape of inverse_H:\", inverse_H.shape)\n",
    "        print(\"Shape of U:\", U.shape)\n",
    "        delta_beta = inverse_H @ U\n",
    "        print(\"Shape of delta_beta:\", delta_beta.shape)\n",
    "        beta = beta + delta_beta\n",
    "\n",
    "        if np.max(np.abs(delta_beta)) < tol:\n",
    "            break\n",
    "    \n",
    "    se = None\n",
    "\n",
    "    if test == 'null_model':\n",
    "        return None, None, ll, None, None\n",
    "    elif test == 'wald':\n",
    "        se = compute_standard_errors(np.linalg.pinv(-H))\n",
    "        stats, pvals = compute_wald_test(beta, se)\n",
    "    elif test == 'lrt':\n",
    "        null_X = np.delete(X, i, axis=1)\n",
    "        _, _, null_ll = firth_logistic_regression(y, null_X, test='null_model')\n",
    "        stats, pvals = compute_likelihood_ratio_test(ll, null_ll)\n",
    "    else:\n",
    "        raise ValueError('Invalid test type')\n",
    "\n",
    "    se = compute_standard_errors(np.linalg.pinv(-H)) if se is None else se\n",
    "    \n",
    "    return beta, se, ll, stats, pvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of inverse_H: (7, 7)\n",
      "Shape of U: (7,)\n",
      "Shape of delta_beta: (7,)\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Array must not contain infs or NaNs.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[239], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m X_ \u001b[38;5;241m=\u001b[39m data[covariates]\u001b[38;5;241m.\u001b[39mto_numpy()\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m      6\u001b[0m y_ \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcase\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy()\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m----> 8\u001b[0m beta, std_err, fitll, wald, pvals \u001b[38;5;241m=\u001b[39m \u001b[43mfirth_logistic_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwald\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m summary \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcovariate\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintercept\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m covariates,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m\"\u001b[39m: beta,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpvals\u001b[39m\u001b[38;5;124m\"\u001b[39m: pvals\n\u001b[0;32m     16\u001b[0m })\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary)\n",
      "Cell \u001b[1;32mIn[238], line 67\u001b[0m, in \u001b[0;36mfirth_logistic_regression\u001b[1;34m(y, X, max_iter, tol, test)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[0;32m     66\u001b[0m     pi \u001b[38;5;241m=\u001b[39m logistic(X \u001b[38;5;241m@\u001b[39m beta)\n\u001b[1;32m---> 67\u001b[0m     H \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_fisher_information\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     ll \u001b[38;5;241m=\u001b[39m compute_log_likelihood(beta, X, y)\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m firth_penalty(ll, H) \u001b[38;5;241m<\u001b[39m tol:\n",
      "File \u001b[1;32mc:\\Users\\bbessell\\Miniconda3\\envs\\bioinf593\\Lib\\site-packages\\numba\\np\\linalg.py:827\u001b[0m, in \u001b[0;36m_check_finite_matrix\u001b[1;34m()\u001b[0m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnditer(a):\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(v\u001b[38;5;241m.\u001b[39mitem()):\n\u001b[1;32m--> 827\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mLinAlgError(\n\u001b[0;32m    828\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArray must not contain infs or NaNs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Array must not contain infs or NaNs."
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../refs/uti_data.csv\", delimiter=\";\").drop(columns=[\"Unnamed: 0\"])\n",
    "data.head()\n",
    "\n",
    "covariates = [\"age\", \"oc\", \"vic\", \"vicl\", \"vis\", \"dia\"]\n",
    "X_ = data[covariates].to_numpy().astype(np.float64)\n",
    "y_ = data[\"case\"].to_numpy().astype(np.float64)\n",
    "\n",
    "beta, std_err, fitll, wald, pvals = firth_logistic_regression(y_, X_, tol=1e-8, test='wald')\n",
    "summary = pd.DataFrame({\n",
    "    \"covariate\": [\"intercept\"] + covariates,\n",
    "    \"beta\": beta,\n",
    "    \"std_err\": std_err,\n",
    "    \"fitll\": fitll,\n",
    "    \"wald\": wald,\n",
    "    \"pvals\": pvals\n",
    "})\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Model fitted by Penalized ML\n",
    "method\n",
    "                   coef  se(coef) lower 0.95  upper 0.95       Chisq            p\n",
    "(Intercept)  0.12025405 0.4763429 -0.8133609  1.05386904  0.06373235 8.006912e-01      1\n",
    "age         -1.10598131 0.4149021 -1.9191744 -0.29278818  7.10565893 7.684097e-03      1\n",
    "oc          -0.06881673 0.4344026 -0.9202301  0.78259664  0.02509593 8.741283e-01      1\n",
    "vic          2.26887464 0.5384872  1.2134590  3.32429026 17.75293469 2.515292e-05      1\n",
    "vicl        -2.11140817 0.5320395 -3.1541864 -1.06862995 15.74913385 7.232104e-05      1\n",
    "vis         -0.78831694 0.4089620 -1.5898677  0.01323384  3.71565877 5.390435e-02      1\n",
    "dia          3.09601166 1.5052197  0.1458353  6.04618801  4.23063350 3.970062e-02\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A working but slow implementation of Firth logistic regression\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import sys\n",
    "from numba import jit, njit\n",
    "from scipy.stats import chi2, norm\n",
    "\n",
    "\n",
    "@njit\n",
    "def firth_likelihood(loglike_val, H):\n",
    "    return -(loglike_val + .5*np.log(np.linalg.det(-H)))\n",
    "\n",
    "\n",
    "@njit\n",
    "def fit_firth_numba(betas, hessian, y, X):\n",
    "    pi = 1 / (  1 + np.exp( -(X @ betas) )  )\n",
    "    W = np.diag(pi * (1 - pi))\n",
    "    cov = np.linalg.pinv(-hessian)\n",
    "\n",
    "    # build hat matrix\n",
    "    rootW = np.sqrt(W)\n",
    "    H = X.T @ rootW.T\n",
    "    H = cov @ H\n",
    "    H = (rootW @ X) @ H\n",
    "\n",
    "    # penalised score\n",
    "                        # h_i\n",
    "    U = X.T @ (y - pi + np.diag(H) * (.5 - pi))\n",
    "    new_betas = betas + cov @ U\n",
    "        \n",
    "    return new_betas\n",
    "\n",
    "\n",
    "def firth_logistic_regression(y : np.array,\n",
    "                              X : np.ndarray,\n",
    "                              max_iter : int = 1000,\n",
    "                              tol : float = 1e-6,\n",
    "                              test : str = 'lrt',\n",
    "                              ) -> tuple[np.array, np.array, float]:\n",
    "    \"\"\"\n",
    "    Perform Firth logistic regression.\n",
    "\n",
    "    beta_hat = argmin | sum(y_i  - pi_i + h_i(1/2 - pi_i))x_{ir} |\n",
    "        i -> N, r -> p = len(beta)\n",
    "\n",
    "    Where: \n",
    "        h_i = ith diagonal element of:\n",
    "            W^(1/2)X(X'WX)^(-1)X'W^(1/2)\n",
    "\n",
    "        |I(beta)|^(1/2) = (X'WX)^(1/2) = Fisher information matrix\n",
    "\n",
    "        pi_i = 1 / (1 + exp(-x_i*beta)) = probability of success\n",
    "        W = diag(pi_i(1-pi_i)) = weight matrix\n",
    "        X = design matrix (n x p) \n",
    "    \"\"\"\n",
    "    X = sm.add_constant(X)\n",
    "    logit_model = sm.Logit(y, X)\n",
    "\n",
    "    # fit null model\n",
    "    null_model = sm.Logit(y, np.ones((len(y), 1)))\n",
    "    null_result = null_model.fit(disp=0)\n",
    "    start_vec = np.zeros(X.shape[1])\n",
    "    start_vec[0] = null_result.params[0]\n",
    "    betas = start_vec\n",
    "\n",
    "    H = logit_model.hessian(betas)\n",
    "    ll = logit_model.loglike(betas)\n",
    "\n",
    "    conv = False\n",
    "    for i in range(max_iter):\n",
    "        new_betas = fit_firth_numba(betas, H, y, X)\n",
    "        ll_next = logit_model.loglike(betas)\n",
    "        H_next = logit_model.hessian(betas)\n",
    "\n",
    "        while firth_likelihood(ll_next, H_next) < firth_likelihood(ll, H):\n",
    "            new_betas = betas + 0.5 * (new_betas - betas)\n",
    "            ll, H = ll_next, H_next\n",
    "            ll_next = logit_model.loglike(new_betas)\n",
    "            H_next = logit_model.hessian(new_betas)\n",
    "            conv = (np.linalg.norm(new_betas - betas) < tol)\n",
    "            if conv: break\n",
    "        \n",
    "        betas = new_betas\n",
    "        if conv: break\n",
    "\n",
    "    if new_betas is None:\n",
    "        sys.stderr.write('Firth regression failed\\n')\n",
    "        return None\n",
    "\n",
    "    # Calculate stats\n",
    "    fitll = ll_next\n",
    "\n",
    "    # add small value to the hessian to ensure it is invertible\n",
    "    H_next += np.eye(H_next.shape[0]) * 1e-6\n",
    "    bse = np.sqrt(np.diag(np.linalg.pinv(-H_next)))\n",
    "\n",
    "    if test == 'null_model':\n",
    "        return None, None, fitll, None, None\n",
    "    \n",
    "    elif test == 'wald':\n",
    "        # Wald test for each coefficient\n",
    "        stats = abs(betas / bse)\n",
    "        pvals = 2 * ( 1 - norm.cdf(stats) )\n",
    "\n",
    "    elif test == 'lrt':\n",
    "        stats, pvals = [], [] # store chi2 stats and pvals\n",
    "        # Likelihood ratio test\n",
    "        for i, bse in enumerate(bse):\n",
    "            null_X = np.delete(X, i, axis=1)\n",
    "\n",
    "            _, _, null_fitll, _, _ = \\\n",
    "                firth_logistic_regression(\n",
    "                    y, null_X, max_iter, tol, test='null_model')\n",
    "            \n",
    "            lr = -2 * (null_fitll - fitll)\n",
    "            stats.append(lr)     \n",
    "            lr_pval = 1 if lr < 0 else chi2.sf(lr, 1)\n",
    "            pvals.append(lr_pval)\n",
    "\n",
    "\n",
    "    return betas, bse, fitll, stats, pvals\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinf593",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
